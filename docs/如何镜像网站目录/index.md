# å¦‚ä½•é•œåƒç½‘ç«™ç›®å½•

â“
> æˆ‘æƒ³é•œåƒç½‘ç«™çš„ä¸€ä¸ªç›®å½•ï¼Œæœ‰å“ªäº›å¥½çš„åŠæ³•ï¼Ÿ æ¯”å¦‚ä¿å­˜https://example.com/path/sub/ ä¸‹çš„å…¨éƒ¨æ–‡ä»¶

ä½ æƒ³â€œé•œåƒç½‘ç«™çš„ä¸€ä¸ªç›®å½•â€ï¼ˆä¾‹å¦‚ `https://example.com/path/sub/`ï¼‰ï¼Œä¹Ÿå°±æ˜¯é€’å½’ä¸‹è½½è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆHTMLã€å›¾ç‰‡ã€CSSã€JS ç­‰ï¼‰ï¼Œå¸¸è§çš„å‡ ç§åšæ³•å¦‚ä¸‹ğŸ‘‡

---

## ğŸ§° ä¸€ã€ç”¨ **wget**ï¼ˆæœ€å¸¸è§ä¹Ÿæœ€å¼ºå¤§çš„æ–¹å¼ï¼‰

Linux / macOS / WSL éƒ½å¯ä»¥ç”¨ï¼ŒWindows ä¹Ÿæœ‰ wgetï¼ˆæˆ–ä½¿ç”¨ PowerShell çš„ wget åˆ«åï¼‰ã€‚

```bash
wget -r -np -nH --cut-dirs=2 -k -E https://example.com/path/sub/
```

**å‚æ•°è§£é‡Šï¼š**

| å‚æ•°             | å«ä¹‰                                         |
| -------------- | ------------------------------------------ |
| `-r`           | é€’å½’ä¸‹è½½ï¼ˆé€’å½’å­é¡µé¢ã€å­ç›®å½•ï¼‰                            |
| `-np`          | ä¸è¿½æº¯åˆ°çˆ¶ç›®å½• (`no-parent`)                      |
| `-nH`          | ä¸åˆ›å»ºä¸»æœºåç›®å½•ï¼ˆé»˜è®¤ä¼šç”Ÿæˆ `example.com/` æ–‡ä»¶å¤¹ï¼‰         |
| `--cut-dirs=2` | å»æ‰è·¯å¾„å‰ 2 å±‚ç›®å½•ï¼Œæ¯”å¦‚ `/path/sub/` å‰é¢ä¸¤å±‚ä¸ä¿ç•™ï¼ˆå¯æŒ‰å®é™…è°ƒï¼‰ |
| `-k`           | è½¬æ¢ HTML é‡Œçš„é“¾æ¥ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„                        |
| `-E`           | ä¸º HTML æ–‡ä»¶åŠ ä¸Š `.html` åç¼€ï¼ˆæ–¹ä¾¿æµè§ˆï¼‰               |

ç»“æœï¼š
ä¼šåœ¨å½“å‰ç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ªæœ¬åœ°åŒ–çš„å‰¯æœ¬ï¼Œå¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨æ‰“å¼€ã€‚

---

## ğŸ§² äºŒã€ç”¨ **HTTrack**ï¼ˆæ›´å¯è§†åŒ–çš„é•œåƒå·¥å…·ï¼‰

é€‚åˆéœ€è¦æ’é™¤ç‰¹å®šè·¯å¾„ã€é™é€Ÿã€å›¾å½¢ç•Œé¢æ“ä½œç­‰æƒ…å†µã€‚

å‘½ä»¤è¡Œç‰ˆæœ¬ï¼š

```bash
httrack "https://example.com/path/sub/" -O "./mirror/" "+https://example.com/path/sub/*" -v
```

æˆ–åœ¨ Windows ä¸‹å®‰è£… [HTTrack Website Copier](https://www.httrack.com/)ï¼Œç”¨ GUI é€‰æ‹©ä¸‹è½½è·¯å¾„ã€‚

---

## ğŸ ä¸‰ã€ç”¨ Python è„šæœ¬ï¼ˆæ›´çµæ´»ï¼‰

å¦‚æœä½ æƒ³ç”¨ Python æ§åˆ¶æˆ–è‡ªå®šä¹‰ä¸‹è½½é€»è¾‘ï¼Œå¯ä»¥ç”¨ `requests + BeautifulSoup` æˆ– `aiohttp` å†™ä¸€ä¸ªçˆ¬è™«ã€‚

ç®€å•ç¤ºä¾‹ï¼ˆåŒæ­¥ç‰ˆï¼‰ï¼š

```python
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

base_url = "https://example.com/path/sub/"
output_dir = "mirror"

os.makedirs(output_dir, exist_ok=True)

def download(url):
    r = requests.get(url)
    if r.status_code == 200:
        path = urlparse(url).path
        if path.endswith('/'):
            path += 'index.html'
        local_path = os.path.join(output_dir, path.lstrip('/'))
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, 'wb') as f:
            f.write(r.content)
        print("âœ…", local_path)
        if 'text/html' in r.headers.get('Content-Type', ''):
            soup = BeautifulSoup(r.text, 'html.parser')
            for link in soup.find_all(['a', 'img', 'script', 'link']):
                href = link.get('href') or link.get('src')
                if href and href.startswith(base_url):
                    download(href)

download(base_url)
```

---

## âš™ï¸ å››ã€ç”¨ **wget + æ­£åˆ™è¿‡æ»¤**

å¦‚æœç½‘ç«™å¾ˆå¤§ä½†ä½ åªæƒ³è¦éƒ¨åˆ†å­ç›®å½•ï¼Œå¯ä»¥ç”¨ï¼š

```bash
wget -r -np -nH -R "*.zip,*.mp4" -A "*.html,*.css,*.js,*.jpg" \
     --no-check-certificate https://example.com/path/sub/
```

è¿™èƒ½æ’é™¤æˆ–åªä¿ç•™æŸäº›ç±»å‹æ–‡ä»¶ã€‚

---

## ğŸš« æ³¨æ„äº‹é¡¹

* æŸäº›ç½‘ç«™ç¦æ­¢é•œåƒï¼ˆé€šè¿‡ robots.txt æˆ–åçˆ¬æœºåˆ¶ï¼‰ã€‚
* å°½é‡ **é™é€Ÿ** å’Œ **åˆç†é—´éš”è¯·æ±‚**ï¼Œé¿å…è¢«å°ã€‚
* å¦‚æœè¯¥ç½‘ç«™éœ€è¦ç™»å½•æˆ– Cookieï¼Œå¯ç”¨ï¼š

  ```bash
  wget --load-cookies cookies.txt ...
  ```

  æˆ– Python requests æºå¸¦ `headers={'Cookie': '...'}`
* é•œåƒå¤§é‡æ–‡ä»¶æ—¶å»ºè®®åŠ  `--limit-rate=100k` æ§åˆ¶é€Ÿç‡ã€‚

---

æ˜¯å¦æ–¹ä¾¿å‘Šè¯‰æˆ‘ï¼š

* ä½ åœ¨ **Linux / Windows / macOS** ä¸Šæ“ä½œï¼Ÿ
* æƒ³ **çº¯å‘½ä»¤è¡Œè‡ªåŠ¨åŒ–** è¿˜æ˜¯ **å¸¦å›¾å½¢ç•Œé¢**ï¼Ÿ
  æˆ‘å¯ä»¥å¸®ä½ å†™ä¸€ä¸ªå®Œå…¨å¯ç›´æ¥è¿è¡Œçš„å‘½ä»¤æˆ–è„šæœ¬ã€‚

éå¸¸å¥½ ğŸ‘Œ é˜¿è’™ã€‚
ä½ è¦çš„â€œç½‘ç«™å­ç›®å½•é•œåƒå·¥å…·â€å¯ä»¥åšåˆ°éå¸¸ç²¾ç»†æ§åˆ¶ï¼š

* æ–­ç‚¹ç»­ä¼ 
* é™åˆ¶ä¸‹è½½ç±»å‹ï¼ˆä¾‹å¦‚åªè¦ `.html`, `.jpg`, `.png`, `.css`, `.js`ï¼‰
* æ—¥å¿—è®°å½•
* é¿å…çˆ¬åˆ°ä¸Šçº§ç›®å½•
* å¯éšæ—¶åœæ­¢/æ¢å¤

ä¸‹é¢æˆ‘åˆ†åˆ«ç»™å‡º **wget å‘½ä»¤æ¨¡æ¿** å’Œ **Python è„šæœ¬æ¨¡æ¿ï¼ˆå®Œæ•´ç‰ˆï¼‰**ã€‚

---

## ğŸš€ ä¸€ã€wget å®Œæ•´é•œåƒå‘½ä»¤æ¨¡æ¿

```bash
wget \
  --mirror \                        # å¼€å¯é•œåƒæ¨¡å¼ï¼ˆé€’å½’ã€æ—¶é—´æˆ³ä¿å­˜ï¼‰
  --convert-links \                 # è½¬æ¢HTMLé“¾æ¥ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
  --adjust-extension \              # è‡ªåŠ¨è¡¥å…….htmlæ‰©å±•å
  --page-requisites \               # ä¸‹è½½æ˜¾ç¤ºç½‘é¡µæ‰€éœ€çš„æ‰€æœ‰èµ„æº
  --no-parent \                     # ä¸çˆ¬åˆ°ä¸Šçº§ç›®å½•
  --continue \                      # æ–­ç‚¹ç»­ä¼ 
  --timestamping \                  # æ¯”è¾ƒæ—¶é—´æˆ³é¿å…é‡å¤ä¸‹è½½
  --restrict-file-names=windows \   # é¿å…éæ³•æ–‡ä»¶åå­—ç¬¦
  --accept=html,htm,jpg,jpeg,png,gif,css,js \   # æŒ‡å®šä¸‹è½½ç±»å‹
  --wait=1 --random-wait \          # æ¯æ¬¡è¯·æ±‚é—´éš”1ç§’ï¼ŒéšæœºåŒ–é˜²å°
  --limit-rate=200k \               # é™é€Ÿé˜²æ­¢è§¦å‘å°ç¦
  --user-agent="Mozilla/5.0 (compatible; SiteMirrorBot/1.0)" \
  --directory-prefix="./mirror_site" \  # æœ¬åœ°ä¿å­˜ç›®å½•
  --output-file=mirror.log \        # æ—¥å¿—æ–‡ä»¶
  --no-check-certificate \          # è·³è¿‡SSLéªŒè¯ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
  https://example.com/path/sub/
```

### âœ… ç”¨æ³•è¯´æ˜

* æ‰§è¡Œä¸€æ¬¡å³å¯å¼€å§‹ä¸‹è½½ã€‚
* åœæ­¢åå¯ç”¨åŒæ ·å‘½ä»¤ç»§ç»­ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰ã€‚
* æ‰€æœ‰æ“ä½œè®°å½•åœ¨ `mirror.log`ã€‚
* æœ¬åœ°ç»“æœåœ¨ `./mirror_site/example.com/path/sub/`ã€‚

---

## ğŸ äºŒã€Python ç½‘ç«™é•œåƒè„šæœ¬æ¨¡æ¿ï¼ˆå¢å¼ºç‰ˆï¼‰

è¿™ä¸ªè„šæœ¬æ›´çµæ´»ï¼Œå¯ä»¥æ§åˆ¶æ–‡ä»¶ç±»å‹ã€æ—¥å¿—æ ¼å¼ã€å¼‚å¸¸å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ ï¼ˆåŸºäºè®°å½•æ–‡ä»¶ï¼‰ã€‚

```python
import os
import time
import json
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

BASE_URL = "https://example.com/path/sub/"
SAVE_DIR = "mirror_site"
ALLOWED_EXTS = {".html", ".htm", ".jpg", ".jpeg", ".png", ".gif", ".css", ".js"}
LOG_FILE = "mirror_log.json"
STATE_FILE = "visited.json"
USER_AGENT = "Mozilla/5.0 (compatible; SiteMirrorBot/1.0)"
DELAY = (1, 3)  # éšæœºå»¶æ—¶åŒºé—´ï¼ˆç§’ï¼‰


# ========== åˆå§‹åŒ– ==========

visited = set()
if os.path.exists(STATE_FILE):
    visited = set(json.load(open(STATE_FILE)))

session = requests.Session()
session.headers.update({"User-Agent": USER_AGENT})


def log_event(event, url, status="ok"):
    entry = {"time": time.strftime("%Y-%m-%d %H:%M:%S"), "event": event, "url": url, "status": status}
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def save_state():
    json.dump(list(visited), open(STATE_FILE, "w"), indent=2)


def should_download(url):
    path = urlparse(url).path
    _, ext = os.path.splitext(path)
    return ext.lower() in ALLOWED_EXTS or ext == ""


def save_file(url, content):
    rel_path = urlparse(url).path.lstrip("/")
    if rel_path.endswith("/") or rel_path == "":
        rel_path += "index.html"
    local_path = os.path.join(SAVE_DIR, rel_path)
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    with open(local_path, "wb") as f:
        f.write(content)
    return local_path


def download(url):
    if url in visited:
        return
    visited.add(url)
    save_state()

    try:
        time.sleep(random.uniform(*DELAY))
        r = session.get(url, timeout=15)
        r.raise_for_status()
        save_file(url, r.content)
        log_event("download", url, "ok")

        if "text/html" in r.headers.get("content-type", ""):
            soup = BeautifulSoup(r.text, "html.parser")
            for tag in soup.find_all(["a", "img", "script", "link"]):
                attr = "href" if tag.name in ["a", "link"] else "src"
                link = tag.get(attr)
                if not link:
                    continue
                abs_url = urljoin(url, link)
                if abs_url.startswith(BASE_URL) and should_download(abs_url):
                    download(abs_url)

    except Exception as e:
        log_event("error", url, str(e))


if __name__ == "__main__":
    import random
    try:
        download(BASE_URL)
    except KeyboardInterrupt:
        print("\nğŸŸ¡ åœæ­¢ä¸‹è½½ï¼Œä¿å­˜çŠ¶æ€ä¸­...")
        save_state()
    print("âœ… å®Œæˆï¼æ—¥å¿—è§ mirror_log.json")
```

---

### âš™ï¸ ä¸»è¦ç‰¹æ€§

| åŠŸèƒ½       | è¯´æ˜                              |
| -------- | ------------------------------- |
| âœ… æ–­ç‚¹ç»­ä¼    | å·²è®¿é—®URLä¿å­˜åœ¨ `visited.json`ï¼Œä¸‹æ¬¡å¯ç»§ç»­ã€‚ |
| âœ… æ–‡ä»¶ç±»å‹è¿‡æ»¤ | ä»…ä¸‹è½½å…è®¸çš„æ‰©å±•åï¼ˆ`ALLOWED_EXTS`ï¼‰ã€‚      |
| âœ… æ—¥å¿—è®°å½•   | æ¯æ¬¡ä¸‹è½½æˆ–é”™è¯¯å†™å…¥ `mirror_log.json`ã€‚    |
| âœ… é¿å…é‡å¤ä¸‹è½½ | `visited` é›†åˆå»é‡ã€‚                 |
| âœ… å»¶æ—¶è®¿é—®   | éšæœºå»¶è¿Ÿé¿å…è§¦å‘å°é”ã€‚                     |
| âœ… å®¹é”™     | è‡ªåŠ¨è·³è¿‡404ã€è¶…æ—¶ã€è¿æ¥é”™è¯¯ã€‚                |

---

## ğŸŒŒ å°è´´å£«

* å¦‚æœç›®æ ‡ç«™ç‚¹æœ‰ç™»å½•æˆ–é˜²çˆ¬ï¼Œå¯ä»¥ç”¨ï¼š

  ```python
  session.cookies.set("sessionid", "xxxx")
  ```

  æˆ–æ‰‹åŠ¨è®¾ç½® `Authorization` å¤´ã€‚
* å¦‚æœæ˜¯ **åŠ¨æ€åŠ è½½å†…å®¹ï¼ˆJSç”Ÿæˆï¼‰**ï¼Œå¯ä»¥æ”¹ç”¨ï¼š

  ```bash
  playwright codegen https://example.com/path/sub/
  ```

  æˆ–ç”¨ `selenium` æŠ“å–æ¸²æŸ“åçš„ HTMLã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ æŠŠè¿™ä¸ª Python è„šæœ¬æ”¹æˆ **å¤šçº¿ç¨‹ç‰ˆï¼ˆåŠ é€Ÿé•œåƒï¼‰** æˆ– **æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯æŒ‡å®šURL/ç±»å‹ï¼‰** çš„ç‰ˆæœ¬ï¼Ÿ

>

å¤ªå¥½äº† ğŸ˜ é˜¿è’™ï¼Œ
ä½ è¿™æ˜¯å¾€â€œ**ç½‘ç«™é•œåƒçˆ¬è™«è¿›åŒ–ç‰ˆ**â€æ–¹å‘è¿ˆè¿›äº†ã€‚
æˆ‘ç»™ä½ å†™ä¸€ä¸ª **å¤šçº¿ç¨‹ + å‘½ä»¤è¡Œå‚æ•°** çš„ Python è„šæœ¬æ¨¡æ¿ï¼Œ
æ—¢èƒ½åŠ é€Ÿä¸‹è½½ï¼Œåˆèƒ½çµæ´»æŒ‡å®šç›®æ ‡URLã€æ–‡ä»¶ç±»å‹ã€çº¿ç¨‹æ•°ã€è¾“å‡ºè·¯å¾„ç­‰ã€‚

---

# âš¡ é«˜æ€§èƒ½ç½‘ç«™é•œåƒçˆ¬è™«ï¼ˆå¤šçº¿ç¨‹ + å‘½ä»¤è¡Œå‚æ•°ç‰ˆï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Website Mirror Tool (Multi-threaded Version)
by Ahmeng's AI Assistant ğŸ›°ï¸

åŠŸèƒ½ï¼š
- å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½
- æ–­ç‚¹ç»­ä¼ 
- æ–‡ä»¶ç±»å‹è¿‡æ»¤
- æ—¥å¿—è®°å½•
- å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
"""

import os
import time
import json
import random
import queue
import argparse
import threading
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# ========== é»˜è®¤é…ç½® ==========
DEFAULT_ALLOWED_EXTS = {".html", ".htm", ".jpg", ".jpeg", ".png", ".gif", ".css", ".js"}
DEFAULT_THREADS = 5
DELAY_RANGE = (0.5, 2.0)  # æ¯ä¸ªçº¿ç¨‹çš„éšæœºå»¶æ—¶

# ========== åˆå§‹åŒ–å‘½ä»¤è¡Œå‚æ•° ==========
parser = argparse.ArgumentParser(description="ğŸ›°ï¸ Mirror a website subdirectory (multi-threaded).")
parser.add_argument("url", help="Base URL to mirror, e.g. https://example.com/path/sub/")
parser.add_argument("-o", "--output", default="mirror_site", help="Output directory (default: mirror_site)")
parser.add_argument("-t", "--threads", type=int, default=DEFAULT_THREADS, help="Number of threads (default: 5)")
parser.add_argument("-a", "--accept", help="Comma-separated allowed extensions, e.g. html,css,js,jpg,png")
parser.add_argument("--user-agent", default="Mozilla/5.0 (compatible; SiteMirrorBot/2.0)",
                    help="Custom user-agent string")
args = parser.parse_args()

BASE_URL = args.url.rstrip("/") + "/"
SAVE_DIR = args.output
THREAD_COUNT = args.threads
ALLOWED_EXTS = {"." + ext.strip().lower() for ext in (args.accept.split(",") if args.accept else DEFAULT_ALLOWED_EXTS)}
LOG_FILE = os.path.join(SAVE_DIR, "mirror_log.jsonl")
STATE_FILE = os.path.join(SAVE_DIR, "visited.json")

# ========== å…¨å±€çŠ¶æ€ ==========
visited_lock = threading.Lock()
visited = set()
q = queue.Queue()
session = requests.Session()
session.headers.update({"User-Agent": args.user_agent})

# ========== å·¥å…·å‡½æ•° ==========

def log_event(event, url, status="ok"):
    entry = {"time": time.strftime("%Y-%m-%d %H:%M:%S"), "event": event, "url": url, "status": status}
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def save_state():
    with visited_lock:
        json.dump(list(visited), open(STATE_FILE, "w", encoding="utf-8"), indent=2)


def should_download(url):
    path = urlparse(url).path
    _, ext = os.path.splitext(path)
    return ext.lower() in ALLOWED_EXTS or ext == ""


def save_file(url, content):
    rel_path = urlparse(url).path.lstrip("/")
    if rel_path.endswith("/") or rel_path == "":
        rel_path += "index.html"
    local_path = os.path.join(SAVE_DIR, rel_path)
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    with open(local_path, "wb") as f:
        f.write(content)
    return local_path


def extract_links(url, html):
    """è§£æHTMLï¼Œæå–ä¸‹çº§URL"""
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for tag in soup.find_all(["a", "img", "script", "link"]):
        attr = "href" if tag.name in ["a", "link"] else "src"
        link = tag.get(attr)
        if not link:
            continue
        abs_url = urljoin(url, link)
        if abs_url.startswith(BASE_URL) and should_download(abs_url):
            links.append(abs_url)
    return links


# ========== ä¸‹è½½çº¿ç¨‹ ==========
def worker():
    while True:
        try:
            url = q.get(timeout=3)
        except queue.Empty:
            return

        with visited_lock:
            if url in visited:
                q.task_done()
                continue
            visited.add(url)
            save_state()

        try:
            time.sleep(random.uniform(*DELAY_RANGE))
            r = session.get(url, timeout=15)
            r.raise_for_status()
            save_file(url, r.content)
            log_event("download", url, "ok")

            if "text/html" in r.headers.get("content-type", ""):
                for link in extract_links(url, r.text):
                    with visited_lock:
                        if link not in visited:
                            q.put(link)

        except Exception as e:
            log_event("error", url, str(e))
        finally:
            q.task_done()


# ========== ä¸»ç¨‹åºå…¥å£ ==========
def main():
    os.makedirs(SAVE_DIR, exist_ok=True)
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r", encoding="utf-8") as f:
                for u in json.load(f):
                    visited.add(u)
            print(f"ğŸŸ¡ æ¢å¤å·²è®¿é—® {len(visited)} ä¸ªURLã€‚")
        except Exception:
            pass

    q.put(BASE_URL)

    threads = [threading.Thread(target=worker, daemon=True) for _ in range(THREAD_COUNT)]
    for t in threads:
        t.start()

    try:
        while any(t.is_alive() for t in threads):
            time.sleep(1)
            if q.empty():
                break
    except KeyboardInterrupt:
        print("\nğŸŸ¡ æ‰‹åŠ¨ä¸­æ–­ï¼Œä¿å­˜çŠ¶æ€ä¸­...")
    finally:
        save_state()
        print(f"âœ… é•œåƒå®Œæˆï¼Œä¿å­˜äº: {SAVE_DIR}")


if __name__ == "__main__":
    main()
```

---

## ğŸ§© ä½¿ç”¨æ–¹æ³•

### 1ï¸âƒ£ æœ€åŸºæœ¬ç”¨æ³•

```bash
python mirror.py https://example.com/path/sub/
```

### 2ï¸âƒ£ æŒ‡å®šè¾“å‡ºè·¯å¾„å’Œçº¿ç¨‹æ•°

```bash
python mirror.py https://example.com/path/sub/ -o ./backup -t 10
```

### 3ï¸âƒ£ é™åˆ¶æ–‡ä»¶ç±»å‹

```bash
python mirror.py https://example.com/path/sub/ -a html,css,js,jpg,png
```

### 4ï¸âƒ£ è‡ªå®šä¹‰ UA

```bash
python mirror.py https://example.com/path/sub/ --user-agent "AhmengBot/1.0"
```

---

## âš™ï¸ ç‰¹æ€§æ€»ç»“

| åŠŸèƒ½         | æè¿°                        |
| ---------- | ------------------------- |
| âš¡ å¤šçº¿ç¨‹      | é»˜è®¤ 5 çº¿ç¨‹ï¼Œå¯è‡ªå®šä¹‰              |
| ğŸ” æ–­ç‚¹ç»­ä¼     | ä¿å­˜è®¿é—®è®°å½•åˆ° `visited.json`    |
| ğŸ¯ ç±»å‹è¿‡æ»¤    | `-a html,css,js,png`      |
| ğŸ§± å¯ä¸­æ–­     | Ctrl+C åœæ­¢åå¯æ¢å¤             |
| ğŸ§¾ æ—¥å¿—      | æ¯ä¸ªäº‹ä»¶å†™å…¥ `mirror_log.jsonl` |
| ğŸ•µï¸â€â™‚ï¸ åå°é” | éšæœºå»¶æ—¶ + è‡ªå®šä¹‰ UA             |
| ğŸ§© å¯æ‰©å±•     | ä½ å¯ä»¥åŠ ç™»å½•ã€ä»£ç†ã€é™é€Ÿç­‰åŠŸèƒ½           |

---

æ˜¯å¦å¸Œæœ›æˆ‘åœ¨è¿™ä¸ªè„šæœ¬çš„åŸºç¡€ä¸Šï¼Œ
å†å¸®ä½ åŠ ä¸€ä¸ª **å¼‚æ­¥ async/await ç‰ˆæœ¬ï¼ˆaiohttp + asyncioï¼‰**ï¼Ÿ
é‚£æ ·èƒ½è¿›ä¸€æ­¥åŠ é€Ÿåˆ°æ•°ç™¾å¹¶å‘è¿æ¥ã€‚
